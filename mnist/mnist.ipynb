{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b80364-540c-4a47-93ae-f7045c600e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics \n",
    "# import bentoml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341c4d0e-0b5a-4d9d-9b63-947cee973bc3",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d14d59-0e26-4f0d-81d3-7fb08f9da8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = gzip.open('../data/mnist.pkl.gz', 'rb')\n",
    "training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d5734c-af1c-4801-b7b1-017f5daf1f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    t = training_data[0][i]\n",
    "    t = t.reshape(-1, 28)\n",
    "    \n",
    "    \n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(t, cmap=\"gray\", interpolation='none')\n",
    "    plt.title(f\"Ground Truth: {training_data[1][i]}\")\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb47c8b-a051-4d78-8046-15c47c665b6f",
   "metadata": {},
   "source": [
    "### SVM for Baseline on MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecaf912-085f-4af9-bf54-c9eca5d55194",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC()\n",
    "clf.fit(training_data[0], training_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45c06c3-c3c5-475c-84d4-22efd81a5018",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [int(a) for a in clf.predict(test_data[0])]\n",
    "num_correct = sum(int(a==y) for a, y in zip(preds, test_data[1]))\n",
    "print(\"Baseline classifier using an SVM.\")\n",
    "print(str(num_correct) + \" of \" + str(len(test_data[1])) + \" values correct.\")\n",
    "print(f\"Accuracy: {num_correct/len(test_data[1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85bc1fe-8c89-42c9-a32a-8afcd74f41fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=100)\n",
    "clf.fit(training_data[0], training_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567a1e16-cb7f-4896-bbbb-65d2774943d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [int(a) for a in clf.predict(test_data[0])]\n",
    "num_correct = sum(int(a==y) for a, y in zip(preds, test_data[1]))\n",
    "print(\"Baseline classifier using a Decision Tree.\")\n",
    "print(str(num_correct) + \" of \" + str(len(test_data[1])) + \" values correct.\")\n",
    "print(f\"Accuracy: {num_correct/len(test_data[1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430becd4-e2ef-4372-919c-68550708ec95",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9d18ee-b06b-406f-97a3-0f790dc9a081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import tensor\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a450b569-0cfb-4092-851f-6a573e1051b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(\n",
    "#   MNIST('./files/', train=True, download=True,\n",
    "#                              transform=Compose([\n",
    "#                                ToTensor(),\n",
    "#                                Normalize(\n",
    "#                                  (0.1307,), (0.3081,))\n",
    "#                              ])),\n",
    "#   batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "# test_loader = DataLoader(\n",
    "#   MNIST('./files/', train=False, download=True,\n",
    "#                              transform=Compose([\n",
    "#                                ToTensor(),\n",
    "#                                Normalize(\n",
    "#                                  (0.1307,), (0.3081,))\n",
    "#                              ])),\n",
    "#   batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509988df-b73c-4416-821d-9e6e319248ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(train_loader))\n",
    "# print(len(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a403091-b296-4e56-a6e6-19fdbd6fc48f",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e40fd5-bc1c-43d8-9239-f241a12fd29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 40\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.008\n",
    "momentum = 0.5\n",
    "log_interval = 100\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602bc313-3ee3-470a-9293-2c2ccbee995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = gzip.open('mnist/DeepLearningPython/mnist.pkl.gz', 'rb')\n",
    "training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfe1455-9bc4-4ee2-b110-66fc22ced4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2d = [im.reshape(-1, 28) for im in training_data[0]]\n",
    "test_2d = [im.reshape(-1, 28) for im in test_data[0]]\n",
    "valid_2d = [im.reshape(-1, 28) for im in validation_data[0]]\n",
    "\n",
    "training_data = TensorDataset(tensor(train_2d), tensor(training_data[1]))\n",
    "train_loader = DataLoader(training_data, batch_size=batch_size_train, shuffle=True)\n",
    "test_data = TensorDataset(tensor(test_2d), tensor(test_data[1]))\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size_test, shuffle=True)\n",
    "# validation_data = TensorDataset(tensor(valid_2d), tensor(validation_data[1]))\n",
    "# validation_loader = DataLoader(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0dcadc-e798-49fa-83ca-f68688bd8cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588d8ea9-2517-44f8-a9bb-5804b7557170",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        # x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d6d79b-16be-4046-a022-7b968f5c8803",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d2352e-9ea4-458d-a9cc-5681ff72a343",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b569711-36c9-491f-9bf5-8f77267502ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs+1)]\n",
    "\n",
    "def train(epoch):\n",
    "    network.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.reshape(-1, 1, 28, 28)\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset),100. * batch_idx / len(train_loader), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append((batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "            torch.save(network.state_dict(), './results/model.pth')\n",
    "            torch.save(optimizer.state_dict(), './results/optimizer.pth')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bd430c-3aaa-430a-9c61-7add42a8ffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.reshape(-1, 1, 28, 28)\n",
    "            output = network(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print(f'\\nTest set: Avg. loss: {test_loss}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset)}%)\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9209b56b-6081-4449-be81-802d02abb863",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6391746a-65be-46dd-a90e-dd4f97bc3f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(train_counter, train_losses, color='blue')\n",
    "plt.scatter(test_counter, test_losses, color='red')\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('negative log likelihood loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e947a9fb-9c7f-4981-a63f-0805c5f5e1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "ex = example_data.reshape(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a99eceb-9430-4ffb-a85d-dfb2ac35f634",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    output = network(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b87664-bef5-4e6f-bcea-071efe095532",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ex[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14ee57c-3b89-4aa5-8926-61e9ada5ba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = network(ex[0])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e013d4-c155-4ce6-928e-80d142817e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f106b914-434d-4b03-b849-4affaa823d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.data.max(1, keepdim=True)[1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef8dd6d-86bc-4b9c-b286-c8f85c9ddd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffe3516-0ad9-4b1e-9639-dc0463df09a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2,5,i+1)\n",
    "    #plt.tight_layout()\n",
    "    plt.imshow(example_data[i], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Prediction: {} Truth: {}\".format(output.data.max(1, keepdim=True)[1][i].item(), example_targets[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21150e4f-e249-4445-a915-ece674c5d2e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
